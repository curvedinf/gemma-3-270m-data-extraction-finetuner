# Evaluation generation configuration
model_output:
  candidate_dir: reports/model_outputs
  prompt_template: configs/prompts/eval_prompt.txt

model:
  path: models/checkpoints/run_latest
  dtype: bfloat16
  max_seq_length: 131072
  device_map: auto
  merge_lora: true
  use_flash_attention_2: false

inference:
  batch_size: 1
  max_new_tokens: 2048
  temperature: 0.0
  top_p: 0.9
  stop_sequences:
    - "</assistant>"
  use_json_grammar: true

datasets:
  validation: data/processed/validation.v1.jsonl
  test: data/processed/test.v1.jsonl

notes: >
  Placeholder inference parameters for running Gemma 3 270M via ROCm-compatible
  runtime before passing outputs to the LLM-as-judge step. Grammar-based decoding
  is enabled to guarantee JSON-valid generations during evaluation.
